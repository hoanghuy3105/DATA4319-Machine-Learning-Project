{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Logistic Regression with Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huy Huynh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA 4319"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes.\n",
    "\n",
    "In simple words, the dependent variable is binary in nature having data coded as either 1 (stands for success/yes) or 0 (stands for failure/no).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependent variable (Y) is binary, that is, it can take only two possible values 0 or 1. Example: If the objective is to determine a given transaction is fraudulent or not, the Y will have a value of 1 if it is fraudulent and 0 if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](https://miro.medium.com/max/1400/0*qDhdxS4TjN_TUJHV.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function/logistic function is a function that resembles an “S” shaped curve when plotted on a graph. It takes values between 0 and 1 and “squishes” them towards the margins at the top and bottom, labeling them as 0 or 1.\n",
    "\n",
    "The equation for the Sigmoid function is this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2](https://miro.medium.com/max/1280/1*OUOB_YF41M-O4GgZH_F2rw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Logistic Regression ###\n",
    "\n",
    "Generally, logistic regression means binary logistic regression having binary target variables, but there can be two more categories of target variables that can be predicted by it. Based on those number of categories, Logistic regression can be divided into following types \n",
    "\n",
    "- Binary or Binomial: when a dependent variable will have only two possible types either 1 and 0. \n",
    "\n",
    "- Multinomial: when dependent variable can have 3 or more possible unordered types or the types having no quantitative significance.\n",
    "\n",
    "- Ordinal: when dependent variable can have 3 or more possible ordered types or the types having a quantitative significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Column1</th><th>gmat</th><th>gpa</th><th>work_experience</th><th>admitted</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Float64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>40 rows × 5 columns</p><tr><th>1</th><td>0</td><td>780</td><td>4.0</td><td>3</td><td>1</td></tr><tr><th>2</th><td>1</td><td>750</td><td>3.9</td><td>4</td><td>1</td></tr><tr><th>3</th><td>2</td><td>690</td><td>3.3</td><td>3</td><td>0</td></tr><tr><th>4</th><td>3</td><td>710</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>5</th><td>4</td><td>680</td><td>3.9</td><td>4</td><td>0</td></tr><tr><th>6</th><td>5</td><td>730</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>7</th><td>6</td><td>690</td><td>2.3</td><td>1</td><td>0</td></tr><tr><th>8</th><td>7</td><td>720</td><td>3.3</td><td>4</td><td>1</td></tr><tr><th>9</th><td>8</td><td>740</td><td>3.3</td><td>5</td><td>1</td></tr><tr><th>10</th><td>9</td><td>690</td><td>1.7</td><td>1</td><td>0</td></tr><tr><th>11</th><td>10</td><td>610</td><td>2.7</td><td>3</td><td>0</td></tr><tr><th>12</th><td>11</td><td>690</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>13</th><td>12</td><td>710</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>14</th><td>13</td><td>680</td><td>3.3</td><td>4</td><td>0</td></tr><tr><th>15</th><td>14</td><td>770</td><td>3.3</td><td>3</td><td>1</td></tr><tr><th>16</th><td>15</td><td>610</td><td>3.0</td><td>1</td><td>0</td></tr><tr><th>17</th><td>16</td><td>580</td><td>2.7</td><td>4</td><td>0</td></tr><tr><th>18</th><td>17</td><td>650</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>19</th><td>18</td><td>540</td><td>2.7</td><td>2</td><td>0</td></tr><tr><th>20</th><td>19</td><td>590</td><td>2.3</td><td>3</td><td>0</td></tr><tr><th>21</th><td>20</td><td>620</td><td>3.3</td><td>2</td><td>1</td></tr><tr><th>22</th><td>21</td><td>600</td><td>2.0</td><td>1</td><td>0</td></tr><tr><th>23</th><td>22</td><td>550</td><td>2.3</td><td>4</td><td>0</td></tr><tr><th>24</th><td>23</td><td>550</td><td>2.7</td><td>1</td><td>0</td></tr><tr><th>25</th><td>24</td><td>570</td><td>3.0</td><td>2</td><td>0</td></tr><tr><th>26</th><td>25</td><td>670</td><td>3.3</td><td>6</td><td>1</td></tr><tr><th>27</th><td>26</td><td>660</td><td>3.7</td><td>4</td><td>1</td></tr><tr><th>28</th><td>27</td><td>580</td><td>2.3</td><td>2</td><td>0</td></tr><tr><th>29</th><td>28</td><td>650</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>30</th><td>29</td><td>660</td><td>3.3</td><td>5</td><td>1</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& Column1 & gmat & gpa & work\\_experience & admitted\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Float64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0 & 780 & 4.0 & 3 & 1 \\\\\n",
       "\t2 & 1 & 750 & 3.9 & 4 & 1 \\\\\n",
       "\t3 & 2 & 690 & 3.3 & 3 & 0 \\\\\n",
       "\t4 & 3 & 710 & 3.7 & 5 & 1 \\\\\n",
       "\t5 & 4 & 680 & 3.9 & 4 & 0 \\\\\n",
       "\t6 & 5 & 730 & 3.7 & 6 & 1 \\\\\n",
       "\t7 & 6 & 690 & 2.3 & 1 & 0 \\\\\n",
       "\t8 & 7 & 720 & 3.3 & 4 & 1 \\\\\n",
       "\t9 & 8 & 740 & 3.3 & 5 & 1 \\\\\n",
       "\t10 & 9 & 690 & 1.7 & 1 & 0 \\\\\n",
       "\t11 & 10 & 610 & 2.7 & 3 & 0 \\\\\n",
       "\t12 & 11 & 690 & 3.7 & 5 & 1 \\\\\n",
       "\t13 & 12 & 710 & 3.7 & 6 & 1 \\\\\n",
       "\t14 & 13 & 680 & 3.3 & 4 & 0 \\\\\n",
       "\t15 & 14 & 770 & 3.3 & 3 & 1 \\\\\n",
       "\t16 & 15 & 610 & 3.0 & 1 & 0 \\\\\n",
       "\t17 & 16 & 580 & 2.7 & 4 & 0 \\\\\n",
       "\t18 & 17 & 650 & 3.7 & 6 & 1 \\\\\n",
       "\t19 & 18 & 540 & 2.7 & 2 & 0 \\\\\n",
       "\t20 & 19 & 590 & 2.3 & 3 & 0 \\\\\n",
       "\t21 & 20 & 620 & 3.3 & 2 & 1 \\\\\n",
       "\t22 & 21 & 600 & 2.0 & 1 & 0 \\\\\n",
       "\t23 & 22 & 550 & 2.3 & 4 & 0 \\\\\n",
       "\t24 & 23 & 550 & 2.7 & 1 & 0 \\\\\n",
       "\t25 & 24 & 570 & 3.0 & 2 & 0 \\\\\n",
       "\t26 & 25 & 670 & 3.3 & 6 & 1 \\\\\n",
       "\t27 & 26 & 660 & 3.7 & 4 & 1 \\\\\n",
       "\t28 & 27 & 580 & 2.3 & 2 & 0 \\\\\n",
       "\t29 & 28 & 650 & 3.7 & 6 & 1 \\\\\n",
       "\t30 & 29 & 660 & 3.3 & 5 & 1 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m40×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Column1 \u001b[0m\u001b[1m gmat  \u001b[0m\u001b[1m gpa     \u001b[0m\u001b[1m work_experience \u001b[0m\u001b[1m admitted \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64   \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64           \u001b[0m\u001b[90m Int64    \u001b[0m\n",
       "─────┼────────────────────────────────────────────────────\n",
       "   1 │       0    780      4.0                3         1\n",
       "   2 │       1    750      3.9                4         1\n",
       "   3 │       2    690      3.3                3         0\n",
       "   4 │       3    710      3.7                5         1\n",
       "   5 │       4    680      3.9                4         0\n",
       "   6 │       5    730      3.7                6         1\n",
       "   7 │       6    690      2.3                1         0\n",
       "   8 │       7    720      3.3                4         1\n",
       "   9 │       8    740      3.3                5         1\n",
       "  10 │       9    690      1.7                1         0\n",
       "  11 │      10    610      2.7                3         0\n",
       "  ⋮  │    ⋮       ⋮       ⋮            ⋮            ⋮\n",
       "  31 │      30    640      3.0                1         0\n",
       "  32 │      31    620      2.7                2         0\n",
       "  33 │      32    660      4.0                4         1\n",
       "  34 │      33    660      3.3                6         1\n",
       "  35 │      34    680      3.3                5         1\n",
       "  36 │      35    650      2.3                1         0\n",
       "  37 │      36    670      2.7                2         0\n",
       "  38 │      37    580      3.3                1         0\n",
       "  39 │      38    590      1.7                4         0\n",
       "  40 │      39    690      3.7                5         1\n",
       "\u001b[36m                                           19 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "using CSV\n",
    "using DataFrames\n",
    "data= CSV.read(\"candidates.csv\",DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Feature\n",
    "Here, you need to divide the given columns into two types of variables dependent(or target variable) and independent variable(or feature variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40-element Vector{Int64}:\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data=[[x[1],x[2]] for x in zip(data.gmat,data.gpa)]\n",
    "y_data=[x for x in data.admitted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_cost (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "σ(x) = 1/(1+exp(-x))\n",
    "\n",
    "function cross_entropy_loss(x,y,w,b)\n",
    "    return -y*log(σ(w'x + b)) - (1-y)*log(1-σ(w'x+b))\n",
    "end \n",
    "\n",
    "function average_cost(features, labels, w, b)\n",
    "    N = length(features)\n",
    "    return (1/N)*sum([cross_entropy_loss(features[i], labels[i],w,b) for i = 1:N])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch_gradient_descent(features,labels,w,b,α)\n",
    "    del_w = [0.0 for i = 1:length(w)]\n",
    "    del_b = 0.0\n",
    "    N = length(features)\n",
    "    for i = 1:N\n",
    "        del_w += (σ(w'features[i] + b) - labels[i])*features[i]\n",
    "        del_b += (σ(w'features[i] + b) - labels[i])\n",
    "    end\n",
    "    w = w - α*del_w\n",
    "    b = b - α*del_b\n",
    "    return w,b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.012, 0.0006000000000000002], -0.0001)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = batch_gradient_descent(x_data, y_data, [0.0,0.0], 0.0, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_batch_gradient_descent(features,labels, w,b,α, epochs)\n",
    "    for i = 1:epochs \n",
    "        \n",
    "        w, b = batch_gradient_descent(features, labels, w,b,α)\n",
    "        if i == 1\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/10\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/8\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/4\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/2\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        end \n",
    "    return w,b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1 with loss 0.6931188566349795\n",
      "Epochs 100000 with loss 0.6855799117618873\n",
      "Epochs 125000 with loss 0.6837589079497152\n",
      "Epochs 250000 with loss 0.6749998518952888\n",
      "Epochs 500000 with loss 0.6590868605720882\n",
      "Epochs 1000000 with loss 0.6326918737673819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0020551903863979, 0.47622113690915635], -0.11626329950708124)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [0.0,0.0]\n",
    "b = 0.0\n",
    "\n",
    "w,b = train_batch_gradient_descent(x_data,y_data, w,b,0.0000001,1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(x,y,w,b)\n",
    "    if σ(w'x+b) >= 0.5\n",
    "        println(\"predict accepted\")\n",
    "        y==1 ? println(\"was accepted\") : println(\"was not accepted\")\n",
    "    else\n",
    "        println(\"predict not accepted\")\n",
    "        y==1 ? println(\"was accepted\") : println(\"was not accepted\")\n",
    "    end \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i =1:length(x_data)\n",
    "    predict(x_data[i],y_data[i],w,b)\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(x,y,w,b)\n",
    "    if σ(w'x+b) >= 0.5\n",
    "        return 1\n",
    "    else\n",
    "        return 0\n",
    "    end \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.275"
     ]
    }
   ],
   "source": [
    "mean_error = 0.0\n",
    "for i = 1:length(x_data)\n",
    "    mean_error += (predict(x_data[i], y_data[i], w, b) - y_data[i])^2\n",
    "end\n",
    "print(mean_error/length(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret\n",
    "\n",
    "The Logistic Regression model can accurately predict if someone will be admitted based on the data and result using the combination of their GPA, GMAT and amount of work experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
